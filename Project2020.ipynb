{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [Linear Regression](#linreg)\n",
    "\n",
    "    2.1 [Preprocessing](#preprocessing)\n",
    "    \n",
    "    2.2 [Algorithm](#algorithm)\n",
    "    \n",
    "    2.3 [Predictions](#predictions)\n",
    "\n",
    "3. [Neural networks](#neural)\n",
    "    \n",
    "    2.1 [Algorithm](#algorithm2)\n",
    "    \n",
    "    2.2 [Predictions](#predictions2)\n",
    "    \n",
    "4. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal of this project is to make 2 predictive models which predict wind turbine power from wind turbine speed. The dataset features and labels (i.e. input and output values) [1] https://medium.com/technology-nineleaps/some-key-machine-learning-definitions-b524eb6cb48 that are both continuous so regression models are employed. The two models are decision tree regression and neural networks respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression\n",
    "This project will use the the DecisionTreeRegressor function from the sklearn package to perform regression using Decision Trees \n",
    "\n",
    "Decision Trees work by breaking down the dataset into smaller and smaller segments while a \"decision tree\" (i.e. a node structure with tests at each node to divide the data) is developed node by node.[1] https://www.saedsayad.com/decision_tree_reg.html\n",
    "\n",
    "This algorithm works in the same way for both classification and regression.\n",
    "\n",
    "\n",
    "### Preprocessing \n",
    "First the data is imported and preprocessed. The preprocessing consists in splitting up the data into feature data (x values) and labels (y values) and rehsaping the X values as the LinearRegression() from linear_model function does not take a 1D array for the X values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "import pandas as pd\n",
    "lin_data = pd.read_csv('powerproduction.csv')\n",
    "lin_data.head()\n",
    "\n",
    "# X and y values for regression\n",
    "X = lin_data.iloc[:, 0].values\n",
    "y = lin_data.iloc[:, 1].values\n",
    "\n",
    "# The X values are reshaped as \n",
    "# they only contain one feature\n",
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing often involves scaling the data and removing outliers etc. The data will not be scaled in this project so the models are as simple as possible and there is no confusion between the predicted data and the observed data. Outliers will not be removed as there is no specific reason given why they are not accurate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "The train_test_split function is defined using the model_selection module from the sklearn package. This is used to randomly split the data into training and testing data.\n",
    "The DecisionTreeRegressor function is defined and used to fit a regressor to the X_train and y_train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "regressor = DecisionTreeRegressor();\n",
    "regressor.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "The predicted values are calculated using the regressor and the X_test testing data. Then various metrics (mean absolute error, mean squared error and root mean squared error) are calculated to test the efficacy of the model. In addition, the coefficient of variation (the root mean squared error RMSE as a percentage of the mean of the observed valeus) is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 6.4924\n",
      "Mean Squared Error: 225.193058\n",
      "Root Mean Squared Error: 15.006433886836673\n",
      "Mean of observed y values: 48.014584\n",
      "Coefficient of variation: 31.253907951877025\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Mean of observed y values:', np.mean(y))\n",
    "# coefficient of variation \n",
    "print('Coefficient of variation:', (100*np.sqrt(metrics.mean_squared_error(y_test, y_pred)))/np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of variation is 31.25%. A good coefficient of variation is considered to be less than 25% [3]. https://www.kw-engineering.com/how-to-assess-a-regressions-predictive-power-energy-use/ so the decision tree regression model is not very accurate at predicting power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "This project will use the the  function from the sklearn package to perform regression using Artificial Neural Networks.\n",
    "\n",
    "Neural networks operate in a similar manner to neurons in the brain [4]. Nodes in the neural network are connected together and they each have an activation function which depends on the inputs (which are the outputs of the other neurons, obtained via the connections). \n",
    "\n",
    "Activation functions differ but generally they are approximately 1 when a certain threshold value has been reached and approximately 0 if that value has not been reached. In addition, the connections between the neurons have weights which amplify or diminish the outputs (in an artificial neural network these are  set to randomly chosen small numbers [5] https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\n",
    "\n",
    "In a typical artificial neural network, there is an input layer of neurons (which have inputs not connected to any other neuron), an output layer (which have outputs not connected to any other neuron) and a hidden layer (neurons which have inputs and outputs connected to other neurons). A deep neural network has multiple hidden layers. [6] https://machinelearningmastery.com/what-is-deep-learning/\n",
    "\n",
    "Neural networks can be used to learn functions and patterns to make predictions. Tensorflow makes predictions using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "The data preprocessed for the decision tree classifier is used again for this.\n",
    "\n",
    "The train_test_split function is defined using the model_selection module from the sklearn package. This is used to randomly split the data into training and testing data.\n",
    "\n",
    "The Input and Dense classes are imported from tensorflow.keras.layers. The input layer and the hidden layers of the neural network are created as objects of these classes.[7] https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/.\n",
    "\n",
    "There are 500 nodes in the hidden layer of the artificial neural network (so this is not a deep neural network as there is only one hidden layer). This is because there are 500 features and 500 labels to train the model on.\n",
    "\n",
    "The activation function in this case is the rectified linear activation function or \"relu\". It is a piecewise linear function that returns the input if the input is positive. If the input is not positive it outputs 0. [8] https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\n",
    "\n",
    "A regression model called model is created using the Model class. It uses the mean_squared error function for the losos function and the Adam algorithm for the optimiser. [9] https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "\n",
    "The model is compiled as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "input_layer = Input(shape=(X.shape[1],))\n",
    "dense_layer_1 = Dense(5000, activation='relu')(input_layer)\n",
    "#dense_layer_2 = Dense(50, activation='relu')(dense_layer_1)\n",
    "#dense_layer_3 = Dense(25, activation='relu')(dense_layer_2)\n",
    "output = Dense(1)(dense_layer_1)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss=\"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on the training data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 1448.1001 - mean_squared_error: 1448.1001 - val_loss: 462.6766 - val_mean_squared_error: 462.6766\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 350.8300 - mean_squared_error: 350.8300 - val_loss: 500.7087 - val_mean_squared_error: 500.7087\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 588.5197 - mean_squared_error: 588.5197 - val_loss: 425.1378 - val_mean_squared_error: 425.1378\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 501.9128 - mean_squared_error: 501.9128 - val_loss: 433.3857 - val_mean_squared_error: 433.3857\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 509.5452 - mean_squared_error: 509.5452 - val_loss: 409.3713 - val_mean_squared_error: 409.3713\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 369.6604 - mean_squared_error: 369.6604 - val_loss: 430.7142 - val_mean_squared_error: 430.7142\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 571.3478 - mean_squared_error: 571.3478 - val_loss: 400.8790 - val_mean_squared_error: 400.8790\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 339.3983 - mean_squared_error: 339.3983 - val_loss: 388.0417 - val_mean_squared_error: 388.0417\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 516.7244 - mean_squared_error: 516.7244 - val_loss: 424.3911 - val_mean_squared_error: 424.3911\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 512.8853 - mean_squared_error: 512.8853 - val_loss: 424.6291 - val_mean_squared_error: 424.6291\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 734.6692 - mean_squared_error: 734.6692 - val_loss: 409.8976 - val_mean_squared_error: 409.8976\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 466.4126 - mean_squared_error: 466.4126 - val_loss: 393.9167 - val_mean_squared_error: 393.9167\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 529.1159 - mean_squared_error: 529.1159 - val_loss: 528.5535 - val_mean_squared_error: 528.5535\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 470.4785 - mean_squared_error: 470.4785 - val_loss: 376.0064 - val_mean_squared_error: 376.0064\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 465.9424 - mean_squared_error: 465.9424 - val_loss: 444.6148 - val_mean_squared_error: 444.6148\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 491.2903 - mean_squared_error: 491.2903 - val_loss: 407.9223 - val_mean_squared_error: 407.9223\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 490.4234 - mean_squared_error: 490.4234 - val_loss: 422.4799 - val_mean_squared_error: 422.4799\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 389.0690 - mean_squared_error: 389.0690 - val_loss: 395.1451 - val_mean_squared_error: 395.1451\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 405.3015 - mean_squared_error: 405.3015 - val_loss: 402.2083 - val_mean_squared_error: 402.2083\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 408.6907 - mean_squared_error: 408.6907 - val_loss: 399.0173 - val_mean_squared_error: 399.0173\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 365.4093 - mean_squared_error: 365.4093 - val_loss: 394.3809 - val_mean_squared_error: 394.3809\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 627.7142 - mean_squared_error: 627.7142 - val_loss: 371.9080 - val_mean_squared_error: 371.9080\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 544.8011 - mean_squared_error: 544.8011 - val_loss: 371.0320 - val_mean_squared_error: 371.0320\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 573.5047 - mean_squared_error: 573.5047 - val_loss: 370.9885 - val_mean_squared_error: 370.9885\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 537.2029 - mean_squared_error: 537.2029 - val_loss: 373.3091 - val_mean_squared_error: 373.3091\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 450.4790 - mean_squared_error: 450.4790 - val_loss: 372.4750 - val_mean_squared_error: 372.4750\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 347.1733 - mean_squared_error: 347.1733 - val_loss: 425.6969 - val_mean_squared_error: 425.6969\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 450.3324 - mean_squared_error: 450.3324 - val_loss: 377.3397 - val_mean_squared_error: 377.3397\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 454.3469 - mean_squared_error: 454.3469 - val_loss: 407.5490 - val_mean_squared_error: 407.5490\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 475.1839 - mean_squared_error: 475.1839 - val_loss: 395.6250 - val_mean_squared_error: 395.6250\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 572.5331 - mean_squared_error: 572.5331 - val_loss: 381.1830 - val_mean_squared_error: 381.1830\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 656.1776 - mean_squared_error: 656.1776 - val_loss: 450.1374 - val_mean_squared_error: 450.1374\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 392.8788 - mean_squared_error: 392.8788 - val_loss: 373.6390 - val_mean_squared_error: 373.6390\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 468.2341 - mean_squared_error: 468.2341 - val_loss: 429.7598 - val_mean_squared_error: 429.7598\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 459.2325 - mean_squared_error: 459.2325 - val_loss: 373.5114 - val_mean_squared_error: 373.5114\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 504.6223 - mean_squared_error: 504.6223 - val_loss: 371.3964 - val_mean_squared_error: 371.3964\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 497.0817 - mean_squared_error: 497.0817 - val_loss: 406.9631 - val_mean_squared_error: 406.9631\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 422.6979 - mean_squared_error: 422.6979 - val_loss: 371.6179 - val_mean_squared_error: 371.6179\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 456.4781 - mean_squared_error: 456.4781 - val_loss: 392.5470 - val_mean_squared_error: 392.5470\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 556.8245 - mean_squared_error: 556.8245 - val_loss: 370.2535 - val_mean_squared_error: 370.2535\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 481.6119 - mean_squared_error: 481.6119 - val_loss: 460.1617 - val_mean_squared_error: 460.1617\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 551.0973 - mean_squared_error: 551.0973 - val_loss: 370.6711 - val_mean_squared_error: 370.6711\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 324.9074 - mean_squared_error: 324.9074 - val_loss: 376.9315 - val_mean_squared_error: 376.9315\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 362.7446 - mean_squared_error: 362.7446 - val_loss: 388.8901 - val_mean_squared_error: 388.8901\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 496.0164 - mean_squared_error: 496.0164 - val_loss: 379.4422 - val_mean_squared_error: 379.4422\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 621.0174 - mean_squared_error: 621.0174 - val_loss: 389.8136 - val_mean_squared_error: 389.8136\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 435.4715 - mean_squared_error: 435.4715 - val_loss: 370.9965 - val_mean_squared_error: 370.9965\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 365.9978 - mean_squared_error: 365.9978 - val_loss: 445.9910 - val_mean_squared_error: 445.9910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 396.7653 - mean_squared_error: 396.7653 - val_loss: 488.0913 - val_mean_squared_error: 488.0913\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 530.5753 - mean_squared_error: 530.5753 - val_loss: 383.0271 - val_mean_squared_error: 383.0271\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 378.0256 - mean_squared_error: 378.0256 - val_loss: 371.9850 - val_mean_squared_error: 371.9850\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 441.2135 - mean_squared_error: 441.2135 - val_loss: 375.7854 - val_mean_squared_error: 375.7854\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 380.0902 - mean_squared_error: 380.0902 - val_loss: 374.8144 - val_mean_squared_error: 374.8144\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 470.0527 - mean_squared_error: 470.0527 - val_loss: 435.3506 - val_mean_squared_error: 435.3506\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 548.3221 - mean_squared_error: 548.3221 - val_loss: 374.6959 - val_mean_squared_error: 374.6959\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 588.4397 - mean_squared_error: 588.4397 - val_loss: 388.3945 - val_mean_squared_error: 388.3945\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 424.5568 - mean_squared_error: 424.5568 - val_loss: 393.2820 - val_mean_squared_error: 393.2820\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 507.4519 - mean_squared_error: 507.4519 - val_loss: 370.6492 - val_mean_squared_error: 370.6492\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 326.1886 - mean_squared_error: 326.1886 - val_loss: 392.5079 - val_mean_squared_error: 392.5079\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 498.0891 - mean_squared_error: 498.0891 - val_loss: 370.1887 - val_mean_squared_error: 370.1887\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 625.0997 - mean_squared_error: 625.0997 - val_loss: 384.6890 - val_mean_squared_error: 384.6890\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 454.1056 - mean_squared_error: 454.1056 - val_loss: 372.7178 - val_mean_squared_error: 372.7178\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 649.8267 - mean_squared_error: 649.8267 - val_loss: 376.1839 - val_mean_squared_error: 376.1839\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 504.1051 - mean_squared_error: 504.1051 - val_loss: 376.4374 - val_mean_squared_error: 376.4374\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 462.7801 - mean_squared_error: 462.7801 - val_loss: 377.3258 - val_mean_squared_error: 377.3258\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 442.6002 - mean_squared_error: 442.6002 - val_loss: 370.4976 - val_mean_squared_error: 370.4976\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 287.1175 - mean_squared_error: 287.1175 - val_loss: 383.8539 - val_mean_squared_error: 383.8539\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 462.1790 - mean_squared_error: 462.1790 - val_loss: 374.4734 - val_mean_squared_error: 374.4734\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 562.5766 - mean_squared_error: 562.5766 - val_loss: 387.5041 - val_mean_squared_error: 387.5041\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 494.6987 - mean_squared_error: 494.6987 - val_loss: 381.7934 - val_mean_squared_error: 381.7934\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 284.9190 - mean_squared_error: 284.9190 - val_loss: 375.7017 - val_mean_squared_error: 375.7017\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 396.1953 - mean_squared_error: 396.1953 - val_loss: 382.7962 - val_mean_squared_error: 382.7962\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 408.7950 - mean_squared_error: 408.7950 - val_loss: 388.1105 - val_mean_squared_error: 388.1105\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 464.4806 - mean_squared_error: 464.4806 - val_loss: 371.3217 - val_mean_squared_error: 371.3217\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 323.0461 - mean_squared_error: 323.0461 - val_loss: 464.3137 - val_mean_squared_error: 464.3137\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 357.5654 - mean_squared_error: 357.5654 - val_loss: 425.5665 - val_mean_squared_error: 425.5665\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 436.4448 - mean_squared_error: 436.4448 - val_loss: 391.2624 - val_mean_squared_error: 391.2624\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 538.7983 - mean_squared_error: 538.7983 - val_loss: 391.5255 - val_mean_squared_error: 391.5255\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 730.5911 - mean_squared_error: 730.5911 - val_loss: 395.0967 - val_mean_squared_error: 395.0967\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 573.9816 - mean_squared_error: 573.9816 - val_loss: 370.5556 - val_mean_squared_error: 370.5556\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 478.3881 - mean_squared_error: 478.3881 - val_loss: 370.8558 - val_mean_squared_error: 370.8558\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - ETA: 0s - loss: 535.4577 - mean_squared_error: 535.45 - 0s 1ms/step - loss: 527.0657 - mean_squared_error: 527.0657 - val_loss: 371.6922 - val_mean_squared_error: 371.6922\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 405.2230 - mean_squared_error: 405.2230 - val_loss: 400.8528 - val_mean_squared_error: 400.8528\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 344.4620 - mean_squared_error: 344.4620 - val_loss: 415.8133 - val_mean_squared_error: 415.8133\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 442.5605 - mean_squared_error: 442.5605 - val_loss: 370.5031 - val_mean_squared_error: 370.5031\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 502.3874 - mean_squared_error: 502.3874 - val_loss: 371.5010 - val_mean_squared_error: 371.5010\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 618.4846 - mean_squared_error: 618.4846 - val_loss: 370.6280 - val_mean_squared_error: 370.6280\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 565.7343 - mean_squared_error: 565.7343 - val_loss: 370.6575 - val_mean_squared_error: 370.6575\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 436.3830 - mean_squared_error: 436.3830 - val_loss: 370.9761 - val_mean_squared_error: 370.9761\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 494.0608 - mean_squared_error: 494.0608 - val_loss: 380.7065 - val_mean_squared_error: 380.7065\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 380.4110 - mean_squared_error: 380.4110 - val_loss: 373.1084 - val_mean_squared_error: 373.1084\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 294.6789 - mean_squared_error: 294.6789 - val_loss: 404.7551 - val_mean_squared_error: 404.7551\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 439.9714 - mean_squared_error: 439.9714 - val_loss: 373.1032 - val_mean_squared_error: 373.1032\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 449.5349 - mean_squared_error: 449.5349 - val_loss: 371.6788 - val_mean_squared_error: 371.6788\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 432.7174 - mean_squared_error: 432.7174 - val_loss: 380.4934 - val_mean_squared_error: 380.4934\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 1ms/step - loss: 467.7923 - mean_squared_error: 467.7923 - val_loss: 375.1318 - val_mean_squared_error: 375.1318\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 438.1107 - mean_squared_error: 438.1107 - val_loss: 371.1527 - val_mean_squared_error: 371.1527\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 381.6895 - mean_squared_error: 381.6895 - val_loss: 372.8512 - val_mean_squared_error: 372.8512\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 400.9613 - mean_squared_error: 400.9613 - val_loss: 369.8664 - val_mean_squared_error: 369.8664\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 537.8495 - mean_squared_error: 537.8495 - val_loss: 369.3985 - val_mean_squared_error: 369.3985\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "The predicted values are calculated using the regressor and the X_test testing data as before. Then various metrics (including the coefficient of variation) are calculated to test the efficacy of the model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 13.807355700556041\n",
      "Mean Squared Error: 475.0402797605675\n",
      "Root Mean Squared Error: 21.79541877919687\n",
      "Mean of observed y values: 48.014584\n",
      "Coefficient of variation: 45.393330449758494\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Mean of observed y values:', np.mean(y))\n",
    "# coefficient of variation \n",
    "print('Coefficient of variation:', (100*np.sqrt(metrics.mean_squared_error(y_test, y_pred)))/np.mean(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
